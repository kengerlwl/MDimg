---
title: "爬虫"
date: 2020-01-15T17:38:17+08:00
draft: false
---

# 爬虫

##### 定义
```
网络爬虫（又称为网页蜘蛛，网络机器人，在FOAF社区中间，更经常的称为网页追逐者），是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本。另外一些不常使用的名字还有蚂蚁、自动索引、模拟程序或者蠕虫。
```
**个人理解就是，从互联网上获取数据，进行自动化，规范化，也就是说，取代人去做繁重的数据采集。再者使用selenium等，可以模拟浏览器，写交互性的自动化脚本，本质都是解放人力**
## 本质
```文本
本质上，爬虫就是获取网页，然后解析网页，最后得到开发者想要的数据。
这么说是不够正确的，或者说，只是爬虫常用的一部分，是对爬虫的一种浅显的理解，实际上，我感觉爬虫就像是模拟浏览器，但是却通过个人的分析，选择需要加载的去加载，获取想要获取的。
```

## 关键点
主要就是两个部分，一个就是定位数据（定位节点），另外一个就是从节点获取数据，或者模拟操作。关于对节点的定位，一种就是普通的通过特殊属性值等来筛选，另外一种就是通过父子节点，兄弟等关系进行推到，因为前端设计的时侯一般是分为几个部分去做的，而且渲染也是在特定的地方进行渲染，所以父子节点的关系，能够良好的对列表进行分析。

## 工具
我个人是常用**python**写爬虫的类型，因为它有很多强大的库，而且性能也很不错。例如使用requests库，非常简单，方便，且强大。然后想要批量，高效率的进行爬虫，可以使用**Scrapy**
去进行项目式的开发，个人感觉是没有明确的界限，当你需要什么就用什么，不要局限于框架。
其次，**java**上的开发就没有那么方便，可能是因为我对java爬虫知之甚少吧。（常用jousp以及正则去进行html解析）

## 关于pythonn爬虫
通过**requests**等库去获取网页，然后解析网页。
```解析的常用方法通过两类
1,是通过id，class，以及其他属性去进行锁定标签。然后提取数据。
2,是通过正则表达式去进行字符串匹配
个人感觉第一种简单，方便。但是第二种同样不可获缺，是必须要进行学习的东西，否则爬虫在某些情况可能回降低写程序的思路。
```
**Scrapy**爬虫框架
```
这个框架的最大优势就是非常的高效率，适用于对于一个网站的各个阶级的页面的爬虫。这些页面之间通常能够形成链式的关系。或者同层的关系
Scrapy多线程并发，效率极高。
```

## 关于反爬虫
对于部分网站是可以直接解析进行爬虫的，但是并不是全部，有些网站针对这种情况进行了防范
- 常见的防范办法是判断**header**请求头,**IP**,以及一些根据反映速度等等鬼才点子进行反爬虫。
```
所以要写一个好爬虫，就需要伪造，学会伪装自己，写好请求头，IP，以及控制反应速度等等。具体代码，可以自行百度
```

## 关于Selenium
这可以说是一个终极武器，
简单的来说，就是真正的去打开一个模拟器，然后加载网页，获取网页数据，

```
有好处也有坏处
好处是可以获取到更加全面的资源，跳过繁重的api接口分析。直接获取加载的数据。
坏处是不加以选择的加载数据，效率极低。
```
综上，虽然有缺点，但是还是挺有用的，至少能够进行方便的浏览器点击，输入等模拟操作，在进行操作自动化的时侯用处很大。





**实现方法是次要的，重要的是思维方式，上层决定下层的运作**

## Seleniumd 的使用技巧
这里强推chrome加上selenium，效率杠杠的

首先解释一下，python是一门**解释性语言**
```
解释性语言定义：

程序不需要编译，在运行程序的时候才翻译，每个语句都是执行的时候才翻译。这样解释性语言每执行一次就需要逐行翻译一次，效率比较低。

现代解释性语言通常把源程序编译成中间代码，然后用解释器把中间代码一条条翻译成目标机器代码，一条条执行。
```

因为selenium的创建时十分耗时的，所以这并不方便于我们开发调试，比如点击某个按键等等。
综上，我们使用console进行开发测试：
![](https://raw.githubusercontent.com/kengerlwl/MDimg/master/image/223e76bc8cf036a7acf3291dcc98752e/b5c4b80561a0ffb5a9f5dc0b539a5c11.png)
这样，可以比如先定位到某个元素，然后边解释，边执行，和**juypter**很像

同时，结合chrome去进行元素的定位
**比如通过css_selector**
```
content = browser.find_element_by_css_selector('#app > div > div.home-page.f-clear > div.home-container > div > div.center-panel > div.card-list > div.feed-card > div.content > div:nth-child(2) > div.main-content > div.card-content > div.post-content > div > div.text.p-rel.description > div')

```
![](https://raw.githubusercontent.com/kengerlwl/MDimg/master/image/223e76bc8cf036a7acf3291dcc98752e/1b5659791b4fb92ac31680e9dc9a3bd7.png)
**或者xpath（也就是dom树**
```
browser.find_element_by_xpath()
```
![](https://raw.githubusercontent.com/kengerlwl/MDimg/master/image/223e76bc8cf036a7acf3291dcc98752e/f48e4af38e3c0a7b4e16f965266adfe7.png)




## 爬虫的工具使用，chrome
最好用的工具之一，就像开发前端一样，可以通过这个查看获取了哪些资源，明白页面节点间的关系。狠方便。

`使用搜索功能搜索数据`
有写网站的数据不是直接静态的写载html中的，现在很多都是动态的用ajax等技术从后端获取，然后利用js渲染好节点数据。 所以怎样知道自己想要的数据在哪个端口呢。

- 在html中查看数据节点的命名方式。（通常会保持一致）
- 利用搜索工具搜索出想要信息，排查。
![](https://raw.githubusercontent.com/kengerlwl/MDimg/master/image/223e76bc8cf036a7acf3291dcc98752e/cbedae4df4dcc92b8165f58fb1ea0046.png)


## 关于数据定位
对于某些网站，他们的数据往往没有那么直观就能再html或者某个json接口中就直接找到，可能他们的数据格式不一样。经过了一定处理，比如四舍五入，或者统计计算（比如我碰到的东方财富网站）。这时候我们就需要对网站进行分析了。要了解其内部js是如何运算数据的，以及最后得出结果。
这里讲一个简单的，对dom树进行监控。这里检测dom树节点什么时候发生变化。
![](https://raw.githubusercontent.com/kengerlwl/MDimg/master/image/223e76bc8cf036a7acf3291dcc98752e/b55ffae2b070b559822bd87e1b421a23.png)
通过这样再元素那对节点进行监控，当节点改变时，就会debug：暂停
![](https://raw.githubusercontent.com/kengerlwl/MDimg/master/image/223e76bc8cf036a7acf3291dcc98752e/49614456be82c270ccfc1305f171e8ad.png)
这样就定位到了js如何变化

还有直接对js进行断点的，但是没有这个好用。

## 分布式，多线程等技术

  使用这些批量的爬虫技术，主要是为了提高效率，因为时间很重要，要在规定时间内将数据又快又好的爬取出来。
- 我主要使用python里面的多线程，协程进行爬取，具体做法
[协程博客](https://hackmd.io/28Kc3q_nR9as7UTbF0Y7Kw)
- 注意爬取的速度， 太快反而会导致错误
- 使用伪装

常用框架：**scrapy**


## 反爬虫
这也很重要，对于爬虫是不可或缺的。



## 分级层爬取，
尽量将数据存储到本地，哪怕其中部分不是我最终需要的数据，只要是中间过程的一步，在不影响整体速度的前提下，尽量将中间数据也存储到本地。

本地数据的读取速度是很快的，重要的是，减少目标服务器的压力。

## 确保数据整体的正确性
很多时候，我们并不能一次性就把握住某个接口的全部特性，那么我们需要尽量的多做测试，在拥有足够多的数据样本的情况下，去进行判断。

明白样本和整体的意义。