# sklearn的库

## 划分训练集，测试集，验证集

```
from sklearn.model_selection import train_test_split

def train_test_val_split(x,y, train_ratio = 0.8,validation_ratio = 0.1,test_ratio = 0.1,random_state=0):
    # random_state for reproduction
    # shuffle must be 'True'
    [x_train, x_test, y_train, y_test] = train_test_split(
x, y, test_size=validation_ratio+test_ratio, random_state=random_state, shuffle=True)

    [x_val, x_test, y_val, y_test] = train_test_split(
    x_test, y_test, test_size=test_ratio/(test_ratio + validation_ratio), random_state=random_state)
    return x_train,y_train, x_test, y_test, x_val, y_val

```

## pytorch加载batch

**先弄一个dataset类**

```
from __future__ import print_function
import torch.utils.data as data
import torch

class MyDataset(data.Dataset):
    def __init__(self, x, y):
        self.x = x
        self.y = y

    def __getitem__(self, index):#返回的是tensor
        x_i, y_i = self.x[index], self.y[index]
        return x_i, y_i

    def __len__(self):
        return len(self.x)

dataset = MyDataset(images, labels)
```

说明

```
在定义torch.utils.data.Dataset的子类时，必须重载的两个函数是__len__和__getitem__。
__len__返回数据集的大小，
__getitem__实现数据集的下标索引，返回对应的图像和标记（不一定非得返回图像和标记，返回元组的长度可以是任意长，这由网络需要的数据决定）。
```

然后可以使用**DataLoader**来载入batch

```
from torch.utils.data import DataLoader
dataloader = DataLoader(MyDataset(images, labels), batch_size=4, shuffle=True, num_workers=0, drop_last=True)       
```

**参数说明**

```
dataset：Dataset类型，从其中加载数据
batch_size：int，可选。每个batch加载多少样本
shuffle：bool，可选。为True时表示每个epoch都对数据进行洗牌
sampler：Sampler，可选。从数据集中采样样本的方法。
num_workers：int，可选。加载数据时使用多少子进程。默认值为0，表示在主进程中加载数据。
collate_fn：callable，可选。
pin_memory：bool，可选
drop_last：bool，可选。True表示如果最后剩下不完全的batch,丢弃。False表示不丢弃。
```



**具体使用说明**

```
for batch_x, batch_y in dataloader:
	print(batch_x.shape, batch_y.shape)

# 或者加入索引index
for step, (batch_x, batch_y) in enumerate(dataloader):
	print(step, batch_x.shape, batch_y.shape)
	pass
```









# 利用index自制加载器

```
import random
import copy


class My_Load():
    """
    利用index索引自制的数据加载器
    """
    def __init__(self, all_mask, ratio = [7, 2, 1]):
        self.all_mask = all_mask
        total = sum(ratio)
        mid1 = (ratio[0]) / total
        mid2 = mid1 + (ratio[1]) / total

        mid1 = int(mid1 * len(all_mask))
        mid2 = int(mid2 * len(all_mask))

        # 利用打算函数，不重合的分割数据
        new_all_mask = copy.deepcopy(all_mask)
        random.shuffle(new_all_mask)
        self.train_mask = new_all_mask[0:mid1]
        self.valid_mask = new_all_mask[mid1:mid2]
        self.test_mask = new_all_mask[mid2:]




    # 随机采样一部分比例数据，相当于minibatch使用
    def get_rand_mask(self,ratio=1,  num=1, mask = None):
        """
        :param ratio: 0.7 随机选择70%的数据
        :param mask: 如果为mask为None，返回all
        :return:
        """
        if ratio != 1:
            len1 = len(mask)
            num = int(len1 * ratio)
            ans = random.sample(mask, num)
            return ans
        else:
            ans = random.sample(mask, num)
            return ans

```







# ref

[pytorch官方](https://pytorch.org/docs/1.13/data.html)

s
