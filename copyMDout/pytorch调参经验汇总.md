## 关于train loss 降低不下去



### 第一步简单的：更换各种参数

**例如，lr, weight_decay, batch,  loss_fun, optim_fun**

看看会不会有有效果的。



**学习率**非常重要的参数

```
神经网络的优化器选取一般选取Adam，但是在有些情况下Adam难以训练，这时候需要使用如SGD之类的其他优化器。学习率决定了网络训练的速度，但学习率不是越大越好，当网络趋近于收敛时应该选择较小的学习率来保证找到更好的最优点。所以，我们需要手动调整学习率，首先选择一个合适的初始学习率，当训练不动之后，稍微降低学习率，然后再训练一段时间，这时候基本上就完全收敛了。一般学习率的调整是乘以/除以10的倍数。不过现在也有一些自动调整学习率的方案了，不过，我们也要知道如何手动调整到合适的学习率。
```

**batch过大不一定是好事**

太小的size可能会导致难以收敏。但是太大的bantch坑你会使得loss过于平均，抗干扰能力差，泛化能力不行。



如果没有，进一步思考。

### 是不是网络本身有效性有问题

一个简单的验证办法是，自己捏造一个肯定有用的数据集，然后将自己得网络放到自己的简单数据集上运行。

看能否学习出一个模型。



### 更换激活函数

不同的激活函数，对模型影响挺大，

Relu可以有效避免梯度消失。利于线性计算

分类用softmax。





### 是不是数据本身有问题

如果数据本身有问题，那么无论如何也训练不出什么东西。



### 查看是不是梯度消失了

查看梯度的代码

```
    for name, parms in model.named_parameters(): 
        print(parms.grad)
```

这个是个细致活。不好办。





## valid loss 降低不下去

如果过拟合，就正则化。

### 适当降低模型的规模

模型复杂度太高，会导致模型容易学到噪声。




## 关于torch的显存机制
需要放入显存的并不是只有模型和数据。
```
1.模型定义：定义了模型的网络结构，产生模型参数；
while(你想训练):
    2.前向传播：执行模型的前向传播，产生中间激活值；
    3.后向传播：执行模型的后向传播，产生梯度；
    4.梯度更新：执行模型参数的更新，第一次执行的时候产生优化器状态。
```
所以在运行中，进程所占用的显存是会有一定波动的。所以最好不要一次性直接放满，不然可能导致会运行着突然溢出了。最好流出一定的显存给中间参数。




# ref

https://blog.ailemon.net/2019/02/26/solution-to-loss-doesnt-drop-in-nn-train/